{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install Required Packages\n",
    "Reinstall the necessary packages using pip to ensure we have a clean start with all dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Required Packages\n",
    "!pip install --upgrade --force-reinstall numpy pandas nltk scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "Import all required libraries for text processing, including NLTK, spaCy, or other NLP libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Functions\n",
    "Create functions to clean and preprocess text, such as tokenization, stopword removal, and sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing Functions\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenize text into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    return words\n",
    "\n",
    "# Function to segment text into sentences\n",
    "def segment_sentences(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences\n",
    "\n",
    "# Example usage\n",
    "example_text = \"This is an example sentence. This is another sentence.\"\n",
    "cleaned_words = preprocess_text(example_text)\n",
    "segmented_sentences = segment_sentences(example_text)\n",
    "\n",
    "print(\"Cleaned Words:\", cleaned_words)\n",
    "print(\"Segmented Sentences:\", segmented_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Scoring Algorithm\n",
    "Implement algorithms to score sentences based on metrics like TF-IDF, sentence position, or word frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Scoring Algorithm\n",
    "\n",
    "# Function to compute TF-IDF scores for sentences\n",
    "def compute_tfidf_scores(sentences):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "    return tfidf_matrix\n",
    "\n",
    "# Function to score sentences based on word frequency\n",
    "def score_sentences_by_word_frequency(sentences):\n",
    "    word_frequencies = {}\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word not in stopwords.words('english') and word not in string.punctuation:\n",
    "                if word not in word_frequencies:\n",
    "                    word_frequencies[word] = 1\n",
    "                else:\n",
    "                    word_frequencies[word] += 1\n",
    "    \n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_frequencies:\n",
    "                if sentence not in sentence_scores:\n",
    "                    sentence_scores[sentence] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_frequencies[word]\n",
    "    \n",
    "    return sentence_scores\n",
    "\n",
    "# Function to score sentences based on their position in the text\n",
    "def score_sentences_by_position(sentences):\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        sentence_scores[sentence] = len(sentences) - i\n",
    "    return sentence_scores\n",
    "\n",
    "# Example usage\n",
    "example_text = \"This is an example sentence. This is another sentence. This is yet another example of a sentence.\"\n",
    "segmented_sentences = segment_sentences(example_text)\n",
    "\n",
    "# Compute TF-IDF scores\n",
    "tfidf_scores = compute_tfidf_scores(segmented_sentences)\n",
    "print(\"TF-IDF Scores:\\n\", tfidf_scores.toarray())\n",
    "\n",
    "# Score sentences by word frequency\n",
    "word_freq_scores = score_sentences_by_word_frequency(segmented_sentences)\n",
    "print(\"Word Frequency Scores:\\n\", word_freq_scores)\n",
    "\n",
    "# Score sentences by position\n",
    "position_scores = score_sentences_by_position(segmented_sentences)\n",
    "print(\"Position Scores:\\n\", position_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summary Generation\n",
    "Develop a function that selects the highest-scoring sentences to form the summary of the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extractive Summary Generation\n",
    "\n",
    "# Function to generate extractive summary\n",
    "def generate_extractive_summary(text, num_sentences=2):\n",
    "    # Segment text into sentences\n",
    "    sentences = segment_sentences(text)\n",
    "    \n",
    "    # Compute TF-IDF scores for sentences\n",
    "    tfidf_scores = compute_tfidf_scores(sentences)\n",
    "    \n",
    "    # Score sentences by word frequency\n",
    "    word_freq_scores = score_sentences_by_word_frequency(sentences)\n",
    "    \n",
    "    # Score sentences by position\n",
    "    position_scores = score_sentences_by_position(sentences)\n",
    "    \n",
    "    # Combine scores (simple sum of scores for demonstration purposes)\n",
    "    combined_scores = {}\n",
    "    for sentence in sentences:\n",
    "        combined_scores[sentence] = (\n",
    "            word_freq_scores.get(sentence, 0) +\n",
    "            position_scores.get(sentence, 0)\n",
    "        )\n",
    "    \n",
    "    # Sort sentences by combined score\n",
    "    sorted_sentences = sorted(combined_scores, key=combined_scores.get, reverse=True)\n",
    "    \n",
    "    # Select top N sentences for summary\n",
    "    summary_sentences = sorted_sentences[:num_sentences]\n",
    "    \n",
    "    # Join selected sentences to form the summary\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example usage\n",
    "example_text = \"This is an example sentence. This is another sentence. This is yet another example of a sentence. This is the fourth sentence in the example.\"\n",
    "summary = generate_extractive_summary(example_text, num_sentences=2)\n",
    "print(\"Extractive Summary:\\n\", summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Summarizer\n",
    "Test the summarizer on different texts and evaluate its performance using metrics such as ROUGE scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the Summarizer\n",
    "\n",
    "# Function to evaluate the summarizer using ROUGE scores\n",
    "from rouge import Rouge\n",
    "\n",
    "def evaluate_summary(reference_summary, generated_summary):\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(generated_summary, reference_summary, avg=True)\n",
    "    return scores\n",
    "\n",
    "# Example texts for testing\n",
    "test_texts = [\n",
    "    {\n",
    "        \"text\": \"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and humans through natural language. The ultimate goal of NLP is to enable computers to understand, interpret, and generate human languages in a way that is both meaningful and useful.\",\n",
    "        \"reference_summary\": \"NLP is a field of AI focused on the interaction between computers and humans through natural language, aiming to enable computers to understand, interpret, and generate human languages.\"\n",
    "    },\n",
    "    {\n",
    "        \"text\": \"Machine learning is a method of data analysis that automates analytical model building. It is a branch of artificial intelligence based on the idea that systems can learn from data, identify patterns, and make decisions with minimal human intervention.\",\n",
    "        \"reference_summary\": \"Machine learning automates analytical model building, allowing systems to learn from data, identify patterns, and make decisions with minimal human intervention.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Test the summarizer and evaluate its performance\n",
    "for i, test in enumerate(test_texts):\n",
    "    print(f\"Test {i+1}:\")\n",
    "    print(\"Original Text:\", test[\"text\"])\n",
    "    generated_summary = generate_extractive_summary(test[\"text\"], num_sentences=2)\n",
    "    print(\"Generated Summary:\", generated_summary)\n",
    "    scores = evaluate_summary(test[\"reference_summary\"], generated_summary)\n",
    "    print(\"ROUGE Scores:\", scores)\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
