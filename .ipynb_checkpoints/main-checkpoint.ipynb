{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries\n",
    "Import all required libraries for text processing, including NLTK, spaCy, or other NLP libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shouv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Shouv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Shouv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Shouv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing Functions\n",
    "Create functions to clean and preprocess text, such as tokenization, stopword removal, and sentence segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing Functions\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from rouge import Rouge\n",
    "import os\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'\\W', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        return text\n",
    "    return \"\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    cleaned = clean_text(text)\n",
    "    words = word_tokenize(cleaned)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]\n",
    "    return filtered_words  # Return words instead of reconstructed sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summary Generation\n",
    "Summarization function using extractive summarization with TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarization function using extractive summarization with TF-IDF\n",
    "def summarize_text(text, max_sentences=5):\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return \"\"\n",
    "        \n",
    "    # Get original sentences\n",
    "    original_sentences = sent_tokenize(text)\n",
    "    if len(original_sentences) <= max_sentences:\n",
    "        return text\n",
    "        \n",
    "    # Preprocess sentences (retain non-empty sentences)\n",
    "    preprocessed_sentences = []\n",
    "    valid_indices = []  # Track indices of non-empty preprocessed sentences\n",
    "    for idx, sentence in enumerate(original_sentences):\n",
    "        processed = preprocess_text(sentence)\n",
    "        if processed:  # Skip empty sentences\n",
    "            preprocessed_sentences.append(' '.join(processed))\n",
    "            valid_indices.append(idx)\n",
    "    \n",
    "    if len(preprocessed_sentences) <= max_sentences:\n",
    "        return text\n",
    "        \n",
    "    # TF-IDF and scoring\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    try:\n",
    "        tfidf_matrix = vectorizer.fit_transform(preprocessed_sentences)\n",
    "    except ValueError:\n",
    "        return text  # Handle empty vocabulary\n",
    "        \n",
    "    similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    sentence_scores = np.sum(similarity_matrix, axis=1)\n",
    "    \n",
    "    # Select top sentences from valid_indices\n",
    "    top_indices = np.argsort(sentence_scores)[-max_sentences:]\n",
    "    selected_original_indices = [valid_indices[i] for i in top_indices]\n",
    "    selected_original_indices = sorted(selected_original_indices)\n",
    "    \n",
    "    summary = ' '.join([original_sentences[i] for i in selected_original_indices])\n",
    "    return summary\n",
    "    \n",
    "# Evaluation function\n",
    "def evaluate_summary(reference_summary, generated_summary):\n",
    "    \"\"\"Evaluate summary using ROUGE scores\"\"\"\n",
    "    rouge = Rouge()\n",
    "    \n",
    "    if not reference_summary or not generated_summary:\n",
    "        return {\"rouge-1\": {\"f\": 0.0}, \"rouge-2\": {\"f\": 0.0}, \"rouge-l\": {\"f\": 0.0}}\n",
    "        \n",
    "    try:\n",
    "        scores = rouge.get_scores(generated_summary, reference_summary)[0]\n",
    "        return scores\n",
    "    except Exception:\n",
    "        return {\"rouge-1\": {\"f\": 0.0}, \"rouge-2\": {\"f\": 0.0}, \"rouge-l\": {\"f\": 0.0}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully with 24985 rows\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    legal_data = pd.read_csv('legal_text_classification.csv')\n",
    "    print(f\"Dataset loaded successfully with {legal_data.shape[0]} rows\")\n",
    "    # Force 'case_text' as the text column\n",
    "    if 'case_text' not in legal_data.columns:\n",
    "        raise ValueError(\"Column 'case_text' not found in dataset\")\n",
    "    text_column = 'case_text'\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()\n",
    "    \n",
    "# Check if the dataset has a 'case_text' column\n",
    "if 'case_text' not in legal_data.columns:\n",
    "    print(\"Error: 'case_text' column not found in the dataset\")\n",
    "    # Handle error or exit\n",
    "else:\n",
    "    text_column = 'case_text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply summarization to the legal texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarizing legal texts...\n"
     ]
    }
   ],
   "source": [
    "print(\"Summarizing legal texts...\")\n",
    "legal_data['summary'] = legal_data[text_column].apply(lambda x: summarize_text(str(x), max_sentences=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "legal_data['original_length'] = legal_data[text_column].apply(lambda x: len(str(x)))\n",
    "legal_data['summary_length'] = legal_data['summary'].apply(len)\n",
    "legal_data['compression_ratio'] = legal_data['summary_length'] / legal_data['original_length']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization complete!\n",
      "Average compression ratio: 0.56\n"
     ]
    }
   ],
   "source": [
    "print(\"Summarization complete!\")\n",
    "print(f\"Average compression ratio: {legal_data['compression_ratio'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to legal_text_summaries.csv\n"
     ]
    }
   ],
   "source": [
    "output_file = 'legal_text_summaries.csv'\n",
    "legal_data.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sample Summaries ---\n",
      "\n",
      "Original text (excerpt): Conveniently, general principles relevant to the Court granting leave to amend pleadings were summarised recently in the judgment of Edmonds J in SPI Spirits (Cyprus) Ltd v Diageo Australia Ltd (No 4)...\n",
      "Summary: Conveniently, general principles relevant to the Court granting leave to amend pleadings were summarised recently in the judgment of Edmonds J in SPI Spirits (Cyprus) Ltd v Diageo Australia Ltd (No 4) [2007] FCA 1035. In relation to this issue I adopt the following statements of his Honour: \" [14] The starting point is that all of such amendments should be made as are necessary to enable the real questions in controversy between the parties to be decided: Queensland v J L Holdings Pty Ltd [1997] HCA 1 ; (1997) 189 CLR 146 ; Dresna Pty Ltd v Misu Nominees Pty Ltd [2003] FCA 1537. [15] The overriding concerns should be to ensure that all matters in issue upon which the parties seek adjudication are determined in the proceeding and to avoid a multiplicity of proceedings: Caruso Australia Pty Ltd v Portec Australia Pty Ltd [1986] FCA 40.\n",
      "Compression ratio: 0.35\n",
      "\n",
      "Original text (excerpt): It is, therefore, hard to resist the temptation of determining such a fundamental issue as authorisation before a full trial of the native title determination application with the very substantial res...\n",
      "Summary: It is only where the application is obviously without merit, that is, where there is no realistic prospect on the material before the Court of the authorisation being shown to have existed at the time it was purportedly granted, that an order will be made summarily dismissing or striking out the main application under s 84C. Where the application is not clearly without merit, so that it is not dismissed summarily or struck out, the Court may consider directing that an application under s 84C be heard and determined at the same time as the main application. The Court may also consider directing that the question of authorisation be heard and determined as a separate question, and be heard and determined prior to the hearing of the main application, under O 29 of the Federal Court Rules .\n",
      "Compression ratio: 0.19\n",
      "\n",
      "Original text (excerpt): Each of Telstra and the Commission submit that its form of declaration encapsulates the result in Telstra No 2 . It is of interest that, in written submissions and correctly, Telstra characterises tha...\n",
      "Summary: At the time of the HomeLine Part increase, Telstra did not increase the Retail Price of any of its other Fixed Voice Services products. d. At the time of the Home Access increase, Telstra did not increase the Retail Price of its Local Services products or any of its other Fixed Voice Services products. f. From at least 5 December 2005 to the present and continuing: 118 Telstra has supplied, and continues to supply: 119 Local Services to Telstra's Wholesale Customers at the Wholesale Price applicable immediately after the Home Access increase; and 120 Fixed Voice Services to Retail Customers at Retail Prices which are materially unchanged from those applicable immediately after the Home Access increase; 121 Telstra has refused, and continues to refuse, to supply: 122 Local Services to Telstra's Wholesale Customers other than at the Wholesale Price applicable immediately after the Home Access increase, except in limited circumstances; and 123 Fixed Voice Services (other than new products introduced since the Home Access increase) to Retail Customers other than at Retail Prices which are materially unchanged from those applicable immediately after the Home Access increase.\n",
      "Compression ratio: 0.07\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Sample Summaries ---\")\n",
    "sample_indices = np.random.choice(legal_data.index, min(3, len(legal_data)), replace=False)\n",
    "for idx in sample_indices:\n",
    "    print(f\"\\nOriginal text (excerpt): {legal_data.loc[idx, text_column][:200]}...\")\n",
    "    print(f\"Summary: {legal_data.loc[idx, 'summary']}\")\n",
    "    print(f\"Compression ratio: {legal_data.loc[idx, 'compression_ratio']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
