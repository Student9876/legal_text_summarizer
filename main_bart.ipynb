{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604e0483-0b7b-4941-8a4f-2c560dd080de",
   "metadata": {},
   "source": [
    "# Install required libraries (if not already installed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c094e54-ab52-4820-b85c-2ebfaa9dda5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
      "Requirement already satisfied: transformers in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (4.50.1)\n",
      "Requirement already satisfied: datasets in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: torch in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: nltk in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (0.20.1+cu121)\n",
      "Requirement already satisfied: bitsandbytes in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (0.45.4)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (2.5.1+cu121)\n",
      "Requirement already satisfied: filelock in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from datasets) (3.11.14)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: click in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from aiohttp->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from aiohttp->datasets) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: colorama in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (4.50.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (2.5.1+cu121)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from transformers[torch]) (1.5.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1.5)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from requests->transformers[torch]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from requests->transformers[torch]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from requests->transformers[torch]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from requests->transformers[torch]) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers datasets torch nltk torchvision bitsandbytes torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install --upgrade accelerate>=0.26.0\n",
    "!pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8539950c-a6e4-4917-b628-f7c47860d178",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97448406-5555-464f-a73e-6482cbd7346a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "from datasets import load_dataset\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc0f3be-5a3c-432e-b777-b0899113e889",
   "metadata": {},
   "source": [
    "# Download sentence tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4972f2c8-9482-455f-be07-a47d3c046d39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Shouv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591a62f-547c-47e9-8115-f0e457e26d9f",
   "metadata": {},
   "source": [
    "# Load the tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b0b85bc-985e-4e11-af8d-3cda3b4cd377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and Tokenizer Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "model_name = 'facebook/bart-base'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model and Tokenizer Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12987fe6-23f1-42e8-bacd-3ad3b75d9d68",
   "metadata": {},
   "source": [
    "# Define a Function for Abstractive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6147b6f-ebd7-45b1-bae2-a86badd8f808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(text, max_input=1024, max_output=200):\n",
    "    \"\"\"\n",
    "    Generate abstractive summary using BART model.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The legal text to summarize.\n",
    "        max_input (int): Max token length for input text.\n",
    "        max_output (int): Max token length for summary output.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated summary.\n",
    "    \"\"\"\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=max_input, truncation=True).to(model.device)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(inputs, max_length=max_output, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    \n",
    "    # Decode and return summary\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba77cb-85d6-47d4-935a-147af7a9eb71",
   "metadata": {},
   "source": [
    "# Test the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c29d37-ae79-4fcf-bbbc-42caa2ffb9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: summarize: In a landmark case, the Supreme Court ruled that freedom of speech does not include the right to incite violence. This decision overturned previous rulings and set a new precedent in constitutional law. The Supreme Court affirmed the First Amendment's First Amendment rights.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"In a landmark case, the Supreme Court ruled that freedom of speech does not include the right to incite violence. \\\n",
    "This decision overturned previous rulings and set a new precedent in constitutional law.\"\n",
    "\n",
    "print(\"Generated Summary:\", generate_summary(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598668e8-2996-4983-b632-a553afc3a7df",
   "metadata": {},
   "source": [
    "# Load a legal dataset (example: 'legal_trec' from Hugging Face datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a58128cc-96c4-46ba-a1a3-d880d9f94331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 287113\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 13368\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 11490\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\") \n",
    "# Rename columns to match \"text\" and \"labels\"\n",
    "dataset = dataset.rename_columns({\"article\": \"text\", \"highlights\": \"labels\"})\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34092dc7-f876-4e6f-bbc2-7bf371ad04c5",
   "metadata": {},
   "source": [
    "# Extract legal texts and their summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69b5f7d2-c8df-4d58-9a8f-2447ec797a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels', 'id'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].select(range(20000))  # Use 100 samples for training\n",
    "dataset[\"validation\"] = dataset[\"validation\"].select(range(1000))  # 20 for validation\n",
    "dataset[\"test\"] = dataset[\"test\"].select(range(1000))  # 20 for testing\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "159d0a4c-14ff-42c1-aed0-6b6d51ffb13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA version: 12.1\n"
     ]
    }
   ],
   "source": [
    "print(f\"CUDA version: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ea612d-d2f3-43ff-bec5-d3ccf06837d9",
   "metadata": {},
   "source": [
    "# Fine-Tune BART on Legal Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6da26dc9-8151-4610-a932-8496b06f26bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab599c6f4c754b4e9e30da611eda9f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85e7014a44547499c7595e1e137150a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "329d09f5f54a49dbacc5e68b46548593",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shouv\\AppData\\Local\\Temp\\ipykernel_15236\\2907722532.py:116: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5000' max='5000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5000/5000 50:39, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.618700</td>\n",
       "      <td>0.483969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.561300</td>\n",
       "      <td>0.474990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.507400</td>\n",
       "      <td>0.479372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.477500</td>\n",
       "      <td>0.478905</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shouv\\miniconda3\\envs\\legal_summarizer\\lib\\site-packages\\transformers\\modeling_utils.py:3353: UserWarning: Moving the following attributes in the config to the generation config: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=5000, training_loss=0.6373593921661377, metrics={'train_runtime': 3039.7854, 'train_samples_per_second': 13.159, 'train_steps_per_second': 1.645, 'total_flos': 2.43894583296e+16, 'train_loss': 0.6373593921661377, 'epoch': 4.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq, AutoModel\n",
    "import torch\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "torch._dynamo.config.suppress_errors = True\n",
    "\n",
    "# def preprocess_data(examples):\n",
    "#     # Tokenize inputs\n",
    "#     model_inputs = tokenizer(\n",
    "#         examples[\"text\"],\n",
    "#         max_length=1024,\n",
    "#         truncation=True,\n",
    "#         padding=\"max_length\"  # Changed from False to max_length\n",
    "#     )\n",
    "    \n",
    "#     # Tokenize targets with the tokenizer\n",
    "#     with tokenizer.as_target_tokenizer():\n",
    "#         labels = tokenizer(\n",
    "#             examples[\"labels\"],\n",
    "#             max_length=200,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\"  # Changed from False to max_length\n",
    "#         )\n",
    "    \n",
    "#     model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "#     return model_inputs\n",
    "\n",
    "def preprocess_data(examples):\n",
    "    # Tokenize inputs and truncate/pad\n",
    "    inputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        max_length=1024,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize targets (summaries)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"labels\"],\n",
    "            max_length=200,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"],\n",
    "        \"attention_mask\": inputs[\"attention_mask\"],\n",
    "        \"labels\": labels[\"input_ids\"]\n",
    "    }\n",
    "    \n",
    "# Tokenize dataset\n",
    "# tokenized_dataset = dataset.map(\n",
    "#     preprocess_data,\n",
    "#     batched=True,\n",
    "#     remove_columns=[\"text\", \"labels\"],\n",
    "#     load_from_cache_file=True\n",
    "# )\n",
    "\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_data,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\", \"labels\", \"id\"],  # Remove all non-tensor columns\n",
    "    load_from_cache_file=False\n",
    ")\n",
    "\n",
    "\n",
    "# Training Arguments Optimized for Your Setup\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./bart_legal_summarizer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=1,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=1,\n",
    "    num_train_epochs=4,\n",
    "    learning_rate=3e-5,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,\n",
    "    optim=\"adamw_bnb_8bit\",\n",
    "    gradient_checkpointing=True,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_num_workers=0,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    remove_unused_columns=False  # Add this line\n",
    ")\n",
    "\n",
    "# Dynamic Padding Collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True  # Enable dynamic padding\n",
    ")\n",
    "\n",
    "# Enable Flash Attention & Memory-Efficient Training\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optionally, create a custom DataLoader for training if you want to experiment with prefetch_factor\n",
    "train_dataloader = DataLoader(\n",
    "    tokenized_dataset[\"train\"],\n",
    "    batch_size=training_args.per_device_train_batch_size,\n",
    "    num_workers=12,\n",
    "    prefetch_factor=4,\n",
    "    pin_memory=True,\n",
    "    collate_fn=data_collator  # Use the data collator for dynamic padding\n",
    ")\n",
    "# Create Trainer using the custom DataLoader for training (if needed)\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Start Training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6091ef38-76fe-4771-b9a1-a17c26afc0f1",
   "metadata": {},
   "source": [
    "# Load the fine tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfaedf1f-1171-4a7e-aebf-d2ac16394d67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): BartScaledWordEmbedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartSdpaAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "# Path to your saved model\n",
    "model_path = \"./bart_legal_summarizer/checkpoint-5000\"  # Replace with your actual path\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained(model_path)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_path)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c165df30-0fbf-4402-a3ca-68c0ef055e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, max_input_length=1024, max_output_length=200):\n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(\n",
    "        \"summarize: \" + text,  # Add task prefix (optional, depends on your training)\n",
    "        max_length=max_input_length,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    # Generate summary\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=max_output_length,\n",
    "        num_beams=10,          # Beam search for better quality\n",
    "        early_stopping=True,  # Stop early if plausible summary is found\n",
    "        length_penalty=1.0    # Encourage longer summaries (adjust as needed)\n",
    "    )\n",
    "    \n",
    "    # Decode and return\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db85b7a6-3ff9-4d07-b9b3-d7605ff615c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def count_words(text):\n",
    "    \"\"\"Count words in a text string.\"\"\"\n",
    "    return len(word_tokenize(text))\n",
    "\n",
    "def calculate_summarization_ratio(original_text, summary_text):\n",
    "    \"\"\"Calculate compression ratio of summary vs original text.\"\"\"\n",
    "    original_length = count_words(original_text)\n",
    "    summary_length = count_words(summary_text)\n",
    "    ratio = summary_length / original_length\n",
    "    return ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b6d907-dcea-4aa6-9426-a66d5ff4e647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarization Ratio: 0.33 (Summary is 32.7% the length of the original)\n",
      "Generated Summary: The general principles governing the exercise of the discretion to award indemnity costs after rejection by an unsuccessful party of a so called Calderbank letter were set out in the judgment of the Full Court in Black v Lipovac .\n",
      "In summary those principles are: 1. Mere refusal of a \"Calderbank offer\" \n",
      "does not itself warrant an order for indemnity .\n"
     ]
    }
   ],
   "source": [
    "# Example legal text\n",
    "legal_text = \"\"\"\n",
    "The general principles governing the exercise of the discretion to award indemnity costs after rejection by an \n",
    "unsuccessful party of a so called Calderbank letter were set out in the judgment of the Full Court in Black v \n",
    "Lipovac [1998] FCA 699 ; (1998) 217 ALR 386. In summary those principles are: 1. Mere refusal of a \"Calderbank offer\" \n",
    "does not itself warrant an order for indemnity costs. In this connection it may be noted that Jessup J in Dais Studio \n",
    "Pty Ltd v Bullet Creative Pty Ltd [2008] FCA 42 said that (at [6]): if the rejection of such an offer is to ground a \n",
    "claim for indemnity costs, it must be by reason of some circumstance other than that the offer happened to comply with \n",
    "the Calderbank principle. 2. To obtain an order for indemnity costs the offeror must show that the refusal to accept it\n",
    "was unreasonable. 3. The reasonableness of the conduct of the offeree is to be viewed in the light of the circumstances \n",
    "that existed when the offer was rejected.\n",
    "\"\"\"\n",
    "\n",
    "# Generate summary using your trained model\n",
    "summary = summarize_text(legal_text)\n",
    "\n",
    "# Calculate ratio\n",
    "ratio = calculate_summarization_ratio(legal_text, summary)\n",
    "print(f\"Summarization Ratio: {ratio:.2f} (Summary is {ratio*100:.1f}% the length of the original)\")\n",
    "summary = summarize_text(legal_text)\n",
    "print(\"Generated Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5654cad7-fcdc-4ad6-8383-313ac0126b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed540b1-b6ea-44cc-a131-4bc90ae9ebe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
